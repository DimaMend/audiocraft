{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you only want to use a pretrained model, you can also directly get it\n",
    "# from the CompressionModel base model class.\n",
    "from audiocraft.models import CompressionModel\n",
    "\n",
    "# Here do not put the `//pretrained/` prefix!\n",
    "model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "\n",
    "import torch\n",
    "import torchaudio \n",
    "import glob\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cuda().eval()\n",
    "\n",
    "# audio_list = [\"backtoblack.mp3\"]\n",
    "# len_signals = 120\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     x_, fs = torchaudio.load(audio_list[0])\n",
    "#     x_ = x_[0, :int(len_signals*fs)]\n",
    "#     if fs != 32000:\n",
    "#         x_ = torchaudio.transforms.Resample(fs, 32000)(x_)\n",
    "\n",
    "#     x = x_.cuda().unsqueeze(0)\n",
    "#     q, scale = model.encode(x) #int codes\n",
    "#     xhat_encodec = model.decode(q, scale).cpu()\n",
    "\n",
    "#     torchaudio.save(f\".tests/prompt.wav\", x.cpu().squeeze().unsqueeze(0), sample_rate=32000, channels_first=True)\n",
    "#     torchaudio.save(f\".tests/encodec.wav\", xhat_encodec.squeeze().unsqueeze(0), sample_rate=32000, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _shuffle_codebooks(x, groups: int = 0):\n",
    "    ''' x is B,N\n",
    "    If groups > 0 we do group shuffling i.e. we only permute between groups.\n",
    "    The shuffling is the same intra group, so that only the groups\n",
    "    will be independent between them, and we keep the intra-group correlation\n",
    "    '''\n",
    "    B, N, dim = x.size()\n",
    "    x_shuffled = torch.zeros_like(x)\n",
    "    groups = groups if groups else N #That way, we keep the original version compatible\n",
    "\n",
    "    assert N%groups == 0, f\"Dimensions {N} are not divisible by number of groups {groups}\"\n",
    "    for group in range(groups):\n",
    "        batch_perm = torch.randperm(B, device=x.device)\n",
    "        x_shuffled[:, group*N//groups: (group+1)*N//groups] = x[batch_perm, group*N//groups: (group+1)*N//groups]\n",
    "    return x_shuffled\n",
    "\n",
    "class MMDLoss(torch.nn.Module):\n",
    "    def __init__(self, delay: bool = False, kernel: str = \"rbf\", scales = [0.1, 1, 5, 10, 20, 50], device=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.delay = delay\n",
    "        self.kernel = kernel\n",
    "        self.scales = scales\n",
    "\n",
    "    def _kernel_fn(self, dxx: torch.Tensor, a: torch.Tensor = None):\n",
    "\n",
    "        if self.kernel == \"rbf\":\n",
    "            return torch.exp((-0.5 / a) * dxx).sum()\n",
    "        \n",
    "        elif self.kernel == \"inverse\":\n",
    "            return (a**2 / (a**2 + dxx)).sum()\n",
    "        \n",
    "        elif self.kernel == \"linear\":\n",
    "            return dxx.sum()\n",
    "\n",
    "        elif self.kernel == \"quadratic\":\n",
    "            return dxx.sum().square()\n",
    "        \n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"inputs is [B, K, D, T].\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "        miniB, K, D, T = inputs.size()\n",
    "        x = inputs.type(torch.float)\n",
    "        x = (x - x.mean(dim=(0, 2, 3), keepdim=True)) / torch.sqrt(x.var(dim=(0, 2, 3), keepdim=True) + 1e-8)\n",
    "\n",
    "        # Reshaping / Delaying\n",
    "        if self.delay:\n",
    "            x = torch.cat([ torch.nn.functional.pad(x[:, delay: (delay+1), :, : T-delay], (delay, 0)) for delay in range(K) ], dim=1)\n",
    "            x = x[..., K: ] #Crop to remove zeros introduced by padding\n",
    "\n",
    "        # Group time dimension and shuffle to sample from factorized distribution\n",
    "        x = rearrange(x, 'b k d t -> (b t) k d')\n",
    "        B = x.shape[0]\n",
    "        y = _shuffle_codebooks(x)\n",
    "        x = x.view(B, -1)\n",
    "        y = y.view(B, -1)\n",
    "\n",
    "        xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "        rx = xx.diag().unsqueeze(0).expand_as(xx)\n",
    "        ry = yy.diag().unsqueeze(0).expand_as(yy)\n",
    "\n",
    "        out: torch.Tensor = 0.0  # type: ignore\n",
    "\n",
    "        if self.kernel == \"rbf\":\n",
    "            dxx = rx.t() + rx - 2.0 * xx\n",
    "            dyy = ry.t() + ry - 2.0 * yy\n",
    "            dxy = rx.t() + ry - 2.0 * zz\n",
    "\n",
    "            for a in self.scales:\n",
    "                out += ( torch.exp((-0.5 / a) * dxx).sum() - B) / (B * (B - 1)) #remove B because that is the sum of the diagonal\n",
    "                out += ( torch.exp((-0.5 / a) * dyy).sum() - B) / (B * (B - 1)) #remove B because that is the sum of the diagonal\n",
    "                out += (-2 / B**2) * torch.exp((-0.5 / a) * dxy).sum()\n",
    "\n",
    "        elif self.kernel == \"inverse\":\n",
    "            dxx = rx.t() + rx - 2.0 * xx\n",
    "            dyy = ry.t() + ry - 2.0 * yy\n",
    "            dxy = rx.t() + ry - 2.0 * zz\n",
    "\n",
    "            for a in self.scales:\n",
    "                out += ( (a**2/(a**2 + dxx)).sum() - B) / (B * (B - 1)) #remove B because that is the sum of the diagonal\n",
    "                out += ( (a**2/(a**2 + dyy)).sum() - B) / (B * (B - 1)) #remove B because that is the sum of the diagonal\n",
    "                out += (-2 / B**2) * (a**2/(a**2 + dxy)).sum()\n",
    "\n",
    "        elif self.kernel == \"linear\": \n",
    "            \n",
    "            out += xx.sum() / (B * (B - 1))\n",
    "            out += yy.sum() / (B * (B - 1))\n",
    "            out += (-2 / B**2) * zz.sum()\n",
    "\n",
    "        elif self.kernel == \"quadratic\": \n",
    "            \n",
    "            out += xx.square().sum() / (B * (B - 1))\n",
    "            out += yy.square().sum() / (B * (B - 1))\n",
    "            out += (-2 / B**2) * zz.square().sum()\n",
    "\n",
    "        return out.clamp(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 6000]) 4\n",
      "1195.1671458296526\n",
      "Scale: [0.1, 1, 5, 10, 20, 50], rbf: 0.9987932443618774\n",
      "3773.5849056603774\n",
      "Scale: 120, rbf: 1.0078104734420776\n",
      "6060.606060606061\n",
      "Scale: 12, inverse: 1.0096665620803833\n",
      "1445.086705202312\n",
      "linear: 0.9977748394012451\n",
      "0.01339405304045004\n",
      "quadratic: 1.0015389919281006\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda().eval()\n",
    "\n",
    "b = 5\n",
    "audio_list = 10*[\"backtoblack.mp3\"]\n",
    "len_signals = 120\n",
    "\n",
    "# loss_rbf = MMDLoss(delay=True, kernel=\"rbf\", scales=[0.1, 1, 5, 10, 20, 50], device=\"cuda\")\n",
    "# loss_inverse = MMDLoss(delay=True, kernel=\"inverse\", scales=[0.1, 1, 5, 10, 20, 50], device=\"cuda\")\n",
    "\n",
    "# loss_inverse = MMDLoss(delay=True, kernel=\"inverse\", scales=[0.1, 1, 5, 10, 20, 50], device=\"cuda\")\n",
    "# loss_lin = MMDLoss(delay=True, kernel=\"linear\", device=\"cuda\")\n",
    "# loss_quad = MMDLoss(delay=True, kernel=\"quadratic\",device=\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    x = []\n",
    "    for i in range(b):\n",
    "\n",
    "        x_, fs = torchaudio.load(audio_list[i])\n",
    "        x_ = x_[0, :int(len_signals*fs)]\n",
    "\n",
    "        if fs != 32000:\n",
    "            x_ = torchaudio.transforms.Resample(fs, 32000)(x_)\n",
    "\n",
    "        x.append(x_)\n",
    "    \n",
    "    x = torch.stack(x).cuda().unsqueeze(1)\n",
    "    q, scale = model.encode(x) #int codes\n",
    "    _, unquantized = model.model.quantizer.decode(q.transpose(0, 1), return_multi=True) #vectors\n",
    "    \n",
    "    # torch.random.manual_seed(1)\n",
    "    \n",
    "    for kernel in [\"ms-rbf\", \"rbf\", \"inverse\", \"linear\", \"quadratic\"]:\n",
    "\n",
    "        if kernel == \"ms-rbf\":\n",
    "            \n",
    "            scales = [0.1, 1, 5, 10, 20, 50]\n",
    "            loss = MMDLoss(delay=True, kernel=\"rbf\", scales=scales, device=\"cuda\")\n",
    "            _loss = loss(unquantized)\n",
    "            _loss *= 1/0.0008367030532\n",
    "            print(1/0.0008367030532)\n",
    "            print(f\"Scale: {scales}, {loss.kernel}: {_loss.item()}\")\n",
    "\n",
    "        if kernel == \"rbf\":\n",
    "            \n",
    "            for scale in [120]:\n",
    "                loss = MMDLoss(delay=True, kernel=kernel, scales=[scale], device=\"cuda\")\n",
    "                _loss = loss(unquantized)\n",
    "                _loss *= 1/0.000265\n",
    "                print(1/0.000265)\n",
    "                print(f\"Scale: {scale}, {loss.kernel}: {_loss.item()}\")\n",
    "\n",
    "        elif kernel == \"inverse\":\n",
    "\n",
    "            # continue\n",
    "            # scale = 12\n",
    "            # for scale in [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]:\n",
    "            for scale in [12]:\n",
    "                loss = MMDLoss(delay=True, kernel=kernel, scales=[scale], device=\"cuda\")\n",
    "                _loss = loss(unquantized)\n",
    "                _loss *= 1/0.000165\n",
    "                print(1/0.000165)\n",
    "                print(f\"Scale: {scale}, {loss.kernel}: {_loss.item()}\")\n",
    "\n",
    "        elif kernel == \"linear\":\n",
    "\n",
    "            loss = MMDLoss(delay=True, kernel=kernel, device=\"cuda\")\n",
    "            _loss = loss(unquantized)\n",
    "            _loss *= 1/0.000692\n",
    "            print(1/0.000692)\n",
    "            print(f\"{loss.kernel}: {_loss.item()}\")\n",
    "\n",
    "        elif kernel == \"quadratic\":\n",
    "\n",
    "            loss = MMDLoss(delay=True, kernel=kernel, device=\"cuda\")\n",
    "            _loss = loss(unquantized)\n",
    "            _loss *= 1/74.66\n",
    "            print(1/74.66)\n",
    "            print(f\"{loss.kernel}: {_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
